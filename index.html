
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Reasoning About Human-Object Interactions Through Dual Attention Networks</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="css/style.css" rel="stylesheet" type="text/css" />
</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr>
      <td width="700" align="center" valign="middle">
      <span class="title">Reasoning About Human-Object Interactions Through Dual Attention Networks</span></td>
    </tr>
  </table>

   <td align="center"><span style="font-size:20px">2019 International Conference on Computer Vision</span></td>

  <p id="authors">
    <br>
    <span style="font-size:20px"><a href="http://tetexiao.com/">Tete Xiao</a></span><sup>1,2</sup>,&nbsp
    <span style="font-size:20px"><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-qfan">Quanfu Fan</a></span><sup>2</sup>,&nbsp
    <span style="font-size:20px"><a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-dgutfre">Dan Gutfreund</a></span><sup>2</sup>,&nbsp
    <span style="font-size:20px"><a href="http://people.csail.mit.edu/mmonfort/">Mathew Monfort</a></span><sup>3</sup>,&nbsp
	<span style="font-size:20px"><a href="http://cvcl.mit.edu/audeoliva.html">Aude Oliva</a></span><sup>3</sup>&nbsp and&nbsp
	<span style="font-size:20px"><a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou</a></span><sup>4</sup><br>
	<sup>1</sup>University of California, Berkeley &nbsp&nbsp&nbsp<sup>2</sup>MIT-IBM Watson AI Lab, IBM Research <br> <sup>3</sup>Massachusetts Institute of Technology &nbsp&nbsp&nbsp<sup>4</sup>The Chinese University of Hong Kong <br>
    <span style="font-size:20px"><a href="./pdf/main.pdf">[Download the Paper]</a></span><br>
  </p>
  
  <br>
  <p><img src="figures/main.png" width="630" align="middle" /></p>
</div>

</br>
<hr>

<div class="container">
  <h2>Abstract</h2>
    <div class="abstract">
        <p>Objects are entities we act upon, where the functionality of an object is determined by how we interact with it. In this work we propose a Dual Attention Network model which reasons about human-object interactions. The dual-attentional framework weights the important features for objects and actions respectively. As a result, the recognition of objects and actions mutually benefit each other. The proposed model shows competitive classification performance on the human-object interaction dataset Something-Something. Besides, it can perform weak spatiotemporal localization and affordance segmentation, despite being trained only with video-level labels. The model not only finds when an action is happening and which object is being manipulated, but also identifies which part of the object is being interacted with.</p>
    </div>
</div>

</br>
<hr>

<div class="container">
  <h2>Framework</h2>
    <div class="framework">
        <p>Our approach exploits the role of human action and object in human-object interactions via the dual attention module. The Dual Attention Network first predicts plausible action and object labels independently as the priors (1st prediction). Then the priors are used to generate attention maps that weight the features of object and action for the 2nd prediction. </p>
        <center><p><img src="figures/framework.png" width="768" align="middle" /></p></center>
    </div>
</div>

</br>
<hr>

<div class="container">
  <h2>Result</h2>
    <div class="result">
        <p><b>Attention maps</b> yielded by the Dual Attention Network with their predicted labels above. The first row is the input frames while the second and third ones are attention maps for recognizing action and object respectively. The model accurately learns the alignment between actions and objects, even when the background is complicated.</p>
        <center><p><img src="figures/attention_maps.png" width="768" align="middle" /></p></center><br>
        <p><b>Visualization of weakly-supervised spatial and temporal localization</b>. Our method find the object being manipulated, as well as the segments in which the action actually happens, despite that it is trained only with video-level labels.</p>
        <center><p><img src="figures/ws_localization.png" width="768" align="middle" /></p></center><br>
        <p><b>Object-affordance segmentation</b>. The model trained with video-level annotations can automatically find object parts associated with possible ongoing actions.</p>
        <center><p><img src="figures/affordance.gif" width="512" align="middle" /></p></center>
    </div>
</div>

</br>
<hr>

<div class="container">
     <center>
      <h2>Demo</h2>
      <table border="0" align="center" cellspacing="0" cellpadding="20">
          <td align="center" valign="middle">
          <iframe width="768" height="540" src="https://www.youtube.com/embed/qOHJhEJ_0CA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </td>
      </table>
      </center>
      This video is recorded by an actor and is not drawn from the dataset that is used to train the model.
</div>

</br>
<hr>

<div class="container">
<!-- <table align=center width=550px> -->
    <table align=center width=800px>
    <center><h2>Paper</h2></center>
    	  <tr>
    		  <td><a href="./pdf/main.pdf"><img class="layered-paper-big" style="height:175px" src="./figures/paper_cover.png"/></a></td>
    		  <td><span style="font-size:13pt">Tete Xiao, Quanfu Fan, Dan Gutfreund, Mathew Monfort, Aude Oliva, Bolei Zhou.<br>
    		      <i>Reasoning About Human-Object Interactions Through Dual Attention Networks</i><br>
                  published at 2019 International Conference on Computer Vision <br>
    		      <a href="https://tetexiao.com">[arXiv]</a>
    		  </td>
              </td>
          </tr>
    </table>
    <br>

  <table align=center width=600px>
	  <tr>
		  <td><span style="font-size:14pt"><center>
		  	<a href="./figures/bibtex.txt">[Bibtex]</a>
          </center></td>
      </tr>
  </table>

</div>

</br>
<hr>

<div class="container">
  <h2>Acknowledgement</h2>
    <div class="acknowledgement">
        <p>This work was supported by the MIT-IBM Watson AI Lab, as well as the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract number D17PC00341. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government.</p>
    </div>
</div>

</br>
<hr>

<div class="containersmall">
    <p>Please contact <a href="mailto:tete_xiao@berkeley.edu">Tete Xiao</a> if you have question.</p>
</div>
 
<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>
